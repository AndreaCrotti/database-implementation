EXERCISE 7
#+SETUPFILE: options.org
#+OPTIONS: \n:t

* TODO 
  DEADLINE: <2009-12-09 Mer>

* Exercise 7.1
  Given two relations R(x, y) and S(y, z) and the following data:
  - T(R) = 1000
  - T(S) = 600
  - V(R, Y) = 150
  - V(S, Y) = 40
  - min(R.y) = min(S.y) = 0 and max(R.y) = max(S.y) = 200
  Where V(R, A) is the number of distinct numbers of attribute A in relation R
  Given the following expressions:
  + \sigma_{y=2}(R)
  + \sigma_{y<3}(R)
  + R \Join S
  
** 1. If we assume uniform distribution, what is the selectivity of each of the expressions?
   - \sigma_{y=2}(R): 
     Assuming uniform distribution we can think that one element every 150 has value 2.
     $F\sigma_{y=2}(R) = 1/150$
# FIXME: check if this is correct
   - \sigma_{y<3}(R): 
#     $F(\sigma_{y < 3}(R)) = F(NOT(\sigma_{y >= 3})(R)) = F(NOT(\sigma_{y > 3} \vee \sigma_{y = 3})(R)) =$
#     $1 - ((3 / 200) + (1 / 150)) = 587/600$ (the two cases ">" and "=" are disjoint)
#     This number is very close to 1, and in fact this selection is "removing" only a small percentage of the data.
     Assuming that R where y=(0,1,2) exist we can simply sum the disjoint selectivities.
     $F(\sigma_{y<3}(R): 3 * (1 / 150) = 1 / 50$
   - R \Join S
     We take the worst case, when the values of y in R are all different from the values of Y in S, so: 
# Not really convinced should maybe be the min not max
     $F(R \Join S) = 1 / max(V(R, y), V(S, y)) = 1 / 150$
     
** 2. If the database holds a histogram of the relations as follows, what are the selectivities?
   - \sigma_{y=2}(R): 
     Simply the number of tuples where y=2 divided by the total number of tuples.
     $F(\sigma_{y=2}(R)) = 100 / 1000 = 1/10$
   - \sigma_{y<3}(R): 
     Counting the tuples where y < 3, so:
     $F(\sigma_{y<3}(R)) = (300 + 120 + 100) / 1000 = 520 / 1000 \approx 1/2$
# FIXME: check if this is correct
   - $R \Join S$: 
     In this case we can only have 10 tuples in our output at maximum.
     Given that in the interval 161-200 we have 600 values and given that in each bucket the distribution is uniform, we can be sure that all those 10 values of y in R will match a value of y in S.
     $F(R \Join S) = 10 / V(R, y) * V(S, y) = 10 / (40 * 150) = 1/600$

** 3. What are the error rates of the first estimation against the second?
    - \sigma_{y=2}(R): 
      $14/15$
    - \sigma_{y<3}(R):
      $24/25$
    - $R \Join S$:
      $1/1200$
      The worst estimation is given by this, because the distribution of the histogram is far from being uniformely distributed.

* Exercise 7.2
  Consider the query $\pi_{A,B,C,D}(R \Join_{A=C} S)$
  - R is 10 pages long
  - R tuples are 300 bytes long
  - S is 100 pages long
  - S tuples are 500 bytes long
  - C is a key for S
  - A is a key for R
  - page size is 1024 bytes
  - combined size of A,B,C,D is 450 bytes

** 1. Cost of writing out the final result
   Given those data we can compute also how many tuples per relation we have.
   We need to use integer division, it's not of course possible to store a fraction of a tuple in one page.
   $T(R) = (1024 / 300) * 10 \approx 30$
   $T(S) = (1024 / 500) * 100 \approx 200$
   During the join operation every tuple of S corresponds to a tuple of R, so we wet
   $T(R \Join_{A=C} S) = 30$
   After the projection we know that the resulting tuple containing the fields "A, B, C, D" has a size of 450 bytes.
   That means that in total to write out the final result we need
   $30 / (1000 / 450) \approx 15$ pages.

** 2. Suppose that three buffer pages are available, the only join method that is implemented is page-oriented nested loops, and 1/10 of the tuples are removed as duplicates after projection.

*** (a) Compute the cost of doing the projection followed by the join.
**** Join
    We choose R as the outer loops given that R has less tuples.
    Using the formula for page-oriented nested loops we get that the cost of the join is
    $C(j) = 10 + 10*100 = 1010$ pages.
# FIXME: does it make any sense? Not using the informations about discarded data
    We then used 30 pages.

**** Sort
     After joining R & S, we have 30 pages result as computed above.
     In the first pass of modified external merge sort algorithm, 30 pages is read in and sorted.
     We do the projection during the sort (modified external merge sort algorithm) and so we use 15 pages as seen before.

**** Merge
     After the projection we only have 15 pages left.
     Supposing simple two-way merge is used here, we need $\log_2(15) = 4$ passes to merge the result.
     In best case 1/10 of the tuples are removed after the first pass of merge, then the cost of first merge would be: $15 + 15*(1-1/10) = 15 + 14 = 29$.
     And the cost of rest 3 passes of merge would be:
     $15 * (1-1/10) * 3 * 2 = 84$ pages.	

**** Total
     The cost of merge in total is $84 + 29 = 113$ pages.
     In worst case, 1/10 of tuples are removed after the last pass of merge, then the cost of merge would be $15 * 3 * 2 + 15 + 15*(1 - 1/ 10) = 119$ pages.

*** (b) Compute the cost of doing the join followed by the projection.
**** Projection
     From 10 pages of 30 original tuples to 6 pages of 30 (A, B)tuples, as 1/10 tuples would be removed at last, we need 30*(1-1/10)/5 = round(5.4) = 6 pages as result.
     Need log2(6) = 3 passes for merge.
     So the total cost of projection R(A,B) is 10 + 6 + 6*3*2 = 52 pages.
     Projection of S(C,D) from 100 pages of 200 original tuples to 50 pages of 200 (C, D)tuples.
     Similar with above, finally we need 200*(1-1/10)/4 = 45 pages as result. Need log2(50) = 6 passes for merge.
     So the total cost of projection S(C,D) is: At best,	(100 + 50) + (50 + 45) + 45*5*2 = 695 pages.
     At worst, (100 + 50) + 50*5*2 + (50 + 45) = 745 pages.

**** Join
     As page-oriented nested loop algorithm is implemented for join, so take the smaller relation R(A, B) as outer loop comparing to S(C, D), we have cost of 6 + 6 * 45 = 276 pages.

**** Total
     Best case: $52 + 695 + 276 = 1023$
     Worst case: $52 + 745 + 276 = 1073$

*** (c) Compute the cost of doing the join first and then the projection on-the-fly.
**** Join and Projection on the fly
     As page-oriented nested loops algorithm is used for join of relations R, S and R has less pages, so take R as the outer loop, the join cost is 10 + 10 * 100 = 1010 pages.

**** Eliminating duplicates
     During the first step, a hash function could be used to filter out the duplicates which should take (A,B,C,D) as a hash key.
     Suppose there is enough buffer to host the all of join result, because we try to do the projection on-the-fly not afterwards.
     So that there would be only 15 more pages needed to write out the result.
     Otherwise, we might need to write out all of rough join result pages first, and then eliminate the duplicates later, which would cost more than 15 pages.

**** Total
     Minimal cost would be $1010 + 15 = 1025$ pages.

*** (d) Would your answers change if 11 buffer pages were available?
    With 11 buffer pages we could have improved dramatically the performance of the sort-merge algorithm.
    Instead of a 2-way buffer as used here we could have used a 9-way buffer, thus needing less passes.
    Changing to 11 buffer then would have given a great benefit to projection (modified external merge sort was used for it).
    But in this case *nothing* would have changed for the join operation, given that it was implemented using page-oriented nested loop algorithm.
