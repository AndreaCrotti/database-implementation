EXERCISE 6
#+SETUPFILE: options.org

* TODO 
 DEADLINE: <2009-12-02 Mer 12:15>

* 1. Selection and projection
** 1. Given the size of a relation (1000 pages), what are the I/O costs for an equality selection on a non-key attribute for the following cases? Assuming each B-tree leaf holds 20 record pointers while each page contains 10 records
   
   In general equality selection must be done scanning the the whole data and checking over the key we need to compare.
   Using indexes or (un)clustered B+Trees can reduce the number of I/O operations.
   Here we have 20 records for B-tree and 10 records for each page, this means that we can't store the whole B-tree in one page, but they can span over more pages.
   The number of matching records should not influence the number of I/O operations needed.

   When relations are not clustered it takes $T(R)$ I/Os instead of $B(R)$ I/Os to read all the tuples in R.

*** (a) with a clustered b-tree of height 3 (matching records are located in one page)
    This is the best possible scenario, a clustered B^{+}-Tree records all the data in a sorted order on disk.
    

*** (b) without any index, nor is the file sorted on the attribute occurring in selection
    This is the worst possible case, we need to scan over all the records and check the equality of the attribute.
    So the number of I/Os needed is just the total number of records, $1000 * 10 = 10000

*** (c) with an unclustered b-tree index of height 3, and there are 10 matching records
    Looking up the index is much cheaper than scanning the whole set of records.
    Here we can use the index on that attribute and then fetch the corresponding page.

*** (d) with an unclustered b-tree of height 3 and one tenth of the records match the selection
    
    
** 2. We assume the relation is of N pages.

*** (a) What is the complexity of sorting-based projection?
    When doing a /projection/ we must
    - remove unnecessary attributes
    - delete all the possible duplicates that we generated
    If we use a sorting-based algorithm then all the duplicates can only be adjacent, and it becomes very easy to detect and delete them.
    The complexity is then $O(M \log{M})$, where for M we denote the number of pages.
    
*** (b) If the records are distributed uniformly, what is the complexity for hash-based projection?
    Uniformity is a good thing for hashing, it allows to define less "indexes".

*** (c) Why is sorting-based projection the standard solution in practice (or why is it superior to hash-based projection)?
    External sorting is quite efficient and we can also modify it to have an even more efficient projection.
    In general the cost is $O(M \log{M})$
    /hash-based/ projections introduce more complexity and don't give any.

* 2. Join
  Consider $Order \Join_{Order.cid=Customer.id} Customer$.
  Cost metric is the number of page I/O.
  - Order contains 10,000 tuples with 10 tuples per page.
  - Customer contains 2,000 tuples and 10 tuples per page.
  - available 52 tuples per buffer
  - each Customer tuple matches 5 tuples in Order
  - it takes 3 I/O operations to access a leaf of $B^{+}- tree$ for Order, 2 I/Os for Customer.

** 1. What is the cost of joining Order and Customer using a page-oriented simple nested loops join?
   The Order relation is the outer relation, while the customer relation is inner. 
   The page number of outer relation is $M = 10000 / 10 = 1000$
   the page number of inner relation is N= 2000/10=200
   Since in this case the join is page-oriented, the cost is:
   $M + M * N = 1000 + 1000 * 200 = 2 * (10^5)$ I/Os
    
** 2. What is the cost of joining Order and Customer using a block nested loops join?
    One buffer page is for scanning inner Customer, so we use (52-2) buffer pages for holding the blocks of outer  Order. 
    The cost is:
    M + [M / (B - 2)] * N = 1000 + [1000 / 50] * 200 = 5000 I/Os
** 3. What is the cost of joining Order and Customer using a sort-merge join?
    First we need to sort the two relations. So we get cost: 10000 * log10000 + 2000 * log2000
    Order is scanned once, each Customer group is scanned once per matching Order tuple.
    So we get the final cost: 10000 * log10000 + 2000 * log2000 + 10000 + 2000 = ?
    
** 4. What is the cost of joining Order and Customer using a hash join?

** 5. If un clustered B+−tree index existed on Order.cid or Customer.id,would either provide a cheaper alternative (index nexted loop join) for performing the join than a block nested loop join?

** 6. Reconsider the question above when the B+ − tree index is clustered.


