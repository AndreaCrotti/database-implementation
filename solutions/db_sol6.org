EXERCISE 6
#+SETUPFILE: options.org

* TODO 
 DEADLINE: <2009-12-02 Mer 12:15>

* 1. Selection and projection
** 1. Given the size of a relation (1000 pages), what are the I/O costs for an equality selection on a non-key attribute for the following cases? Assuming each B-tree leaf holds 20 record pointers while each page contains 10 records
   
   In general equality selection must be done scanning the whole data and checking over the key we need to compare.
   Using indexes or (un)clustered B+-trees can reduce the number of I/O operations.
   Here we have 20 records for B+-tree and 10 records for each page, this means that we can't store the whole B-tree in one page, but they can span over more pages.
   The number of matching records should not influence the number of I/O operations needed.

   When relations are not clustered it takes $T(R)$ I/Os instead of $B(R)$ I/Os to read all the tuples in R.

*** (a) with a clustered b-tree of height 3 (matching records are located in one page)
    This is the best scenario, a clustered B+Tree records all the data in a sorted order on disk.
    I/O $cost = h + 1 = 4$, we only need traverse the tree from root to leaf and then read the data page.

*** (b) without any index, nor is the file sorted on the attribute occurring in selection
    This is the worst possible case, we need to scan over all the records and check the equality of the attribute.
    So the number of I/Os needed is the total number of pages of the relation.
    I/O $cost = 1000$

*** (c) with an unclustered b-tree index of height 3, and there are 10 matching records
    We scan through the tree and in plus we must read the pages which this time are not contiguous as before.
    So we get
    I/O $cost = h + k = 13$

*** (d) with an unclustered b-tree of height 3 and one tenth of the records match the selection
    Same as before, having more records matching the selection makes the unclustered b-tree much less convenient.
    I/O $cost = h + #tuples/10 = 3 + 1000 = 10003$
    If the matching rate is higher than it's even faster to do a total lookup on the pages.
    
** 2. We assume the relation is of N pages.

*** (a) What is the complexity of sorting-based projection?
    When doing a /projection/ we must
    - remove unnecessary attributes
    - delete all the possible duplicates that we generated
    If we use a sorting-based algorithm then all the duplicates can only be adjacent, and it becomes very easy to detect and delete them.
    The complexity is then $O(N \log{N})$, where for N we denote the number of pages.
    
*** (b) If the records are distributed uniformly, what is the complexity for hash-based projection?
# FIXME: this one can be written better
    For hash-based projection the best-case is $O(N)$.
    To be more precise we have to compose the cost of partitioning and comparing, so we get
    $partCost + inBucketCompCost = (N + N) + N = 3N$
    To have good performances with hash-based projections the buckets must fit in memory.

*** (c) Why is sorting-based projection the standard solution in practice (or why is it superior to hash-based projection)?
    In general a sorting-based projection is used because:
    - sorting is a basic routine in database implementations and the code is fairly optimized
    - hashing can fail if the bucket is too large
    - hashing-based projection can become very slow if the data are not distributed uniformly

* 2. Join
  Consider $Order \Join_{Order.cid=Customer.id} Customer$.
  Cost metric is the number of page I/O.
  - Order contains 10,000 tuples with 10 tuples per page.
  - Customer contains 2,000 tuples and 10 tuples per page.
  - available 52 buffer pages
  - each Customer tuple matches 5 tuples in Order
  - it takes 3 I/O operations to access a leaf of $B^{+}- tree$ for Order, 2 I/Os for Customer.

  Formalizing the following data are given:
  - M = 1000 (number of pages in /Order/)
  - N = 200 (number of pages in /Customer/)
  - Po = 10 (number of tuples/page in /Order/)
  - Pc = 10 (number of tuples/page in /Customer/)
  - B = 52 (number of buffer pages)

** 1. What is the cost of joining Order and Customer using a page-oriented simple nested loops join?
   The Order relation is the outer relation, while the customer relation is inner. 
   Since in this case the join is page-oriented, the cost is:
   $M + (Po * M * N) = 1000 + (10 * 1000 * 200) \approx on 2 * (10^6)$ I/Os
    
** 2. What is the cost of joining Order and Customer using a block nested loops join?
   Having 
   One buffer page is for scanning /Customer/ (inner relation), so we use (52-2) buffer pages for holding the blocks of /Order/ (outer relation).
   The cost is:
   $M + \lceil M / (B - 2)\rceil * N = 1000 + \lceil 1000 / 50 \rceil * 200 = 5000$ I/Os

** 3. What is the cost of joining Order and Customer using a sort-merge join?
   First we need to sort the two relations. So we get cost: 10000 * log10000 + 2000 * log2000
   Order is scanned once, each Customer group is scanned once per matching Order tuple.
   So we get the final cost: $M * logM + N * logN + M + N = 1000 * log1000 + 200 * log200 + 1200$ I/0s
    
** 4. What is the cost of joining Order and Customer using a hash join?
   We first partition both relations, which means we read and write both relations, so we have cost of 2 * (M + N) = 2 * 1200 I/0s. Then we scan matching partitions, read both relations, so we have cost of  1000 + 200 I/Os. In total, we have cost 3600 I/Os.

** 5. If unclustered B+tree index existed on Order.cid or Customer.id,would either provide a cheaper alternative (index nexted loop join) for performing the join than a block nested loop join?
   A B+tree for Order would have height $h = \log_2 10000 \approx 13$.
# Don't know how to compute how many values I should match.
   For index nested loop join:
   Since it is unclustered B^{+}−tree, for each Order tuple, cost of probing Customer index is 3, and Cost of then finding S tuples is upto 1 I/O per matching S tuple. The cost is M + ( (M*pR) * cost of index and data access ) = 1000 + 10000 * (3 + 1) = 41000 I/Os
   For a block nested loop join, we need 2 * 2 * M + 2 * 2 * N + M + N = 4000 + 800 + 1200 = 6000 I/0s
# I am not sure whether we should compare them like this. Isn't it too obvious? I also don't understand and not sure about the 2*2 in block nested loop join
** 6. Reconsider the question above when the B^{+}−tree index is clustered.
   If the B^{+}-tree index is clustered we only have the cost of scanning the height of the tree.
# not really sure here either

   
